{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Import statement\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "id": "qG3FIaNALRyO",
    "ExecuteTime": {
     "end_time": "2023-06-26T13:30:53.708938Z",
     "start_time": "2023-06-26T13:30:53.677236Z"
    }
   },
   "outputs": [],
   "source": [
    "#import dependencies\n",
    "import os, urllib, io\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import torch, torchvision\n",
    "from torch import nn, optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Functions for training and evaluation process"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# function for the training process\n",
    "def model_training(in_model, device, learning_rate, num_epochs, minibatch_size, train_data, models_directory):\n",
    "    model = in_model.to(device)\n",
    "\n",
    "    # print the class name of the model\n",
    "    print('[LOG] Training {}...'.format(str(model.__class__.__name__)))\n",
    "\n",
    "    # print the initialized architectures\n",
    "    print('[LOG] Model architecture:\\n\\n{}\\n'.format(model))\n",
    "\n",
    "    # init the number of model parameters\n",
    "    num_params = 0\n",
    "\n",
    "    # iterate over the distinct parameters\n",
    "    for param in model.parameters():\n",
    "\n",
    "        # collect number of parameters\n",
    "        num_params += param.numel()\n",
    "\n",
    "    # print the number of model parameters\n",
    "    print(f'[LOG] Number of to be trained {str(model.__class__.__name__)} model parameters: {num_params}\\n')\n",
    "\n",
    "    # define the optimization criterion / loss function\n",
    "    nll_loss = nn.NLLLoss()\n",
    "    nll_loss = nll_loss.to(device)\n",
    "\n",
    "    # define learning rate and optimization strategy\n",
    "    learning_rate = learning_rate\n",
    "    optimizer = optim.SGD(params=model.parameters(), lr=learning_rate)\n",
    "    # specify the training parameters\n",
    "    num_epochs = num_epochs # number of training epochs\n",
    "    mini_batch_size = minibatch_size # size of the mini-batches\n",
    "    fashion_mnist_train_dataloader = torch.utils.data.DataLoader(train_data, batch_size=mini_batch_size, shuffle=True)\n",
    "\n",
    "    # init collection of training epoch losses\n",
    "    train_epoch_losses = []\n",
    "\n",
    "    # set the model in training mode\n",
    "    model.train()\n",
    "\n",
    "    # train the FashionMNISTNet model\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        # init collection of mini-batch losses\n",
    "        train_mini_batch_losses = []\n",
    "\n",
    "        # iterate over all-mini batches\n",
    "        for i, (images, labels) in enumerate(fashion_mnist_train_dataloader):\n",
    "\n",
    "            # push mini-batch data to computation device\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # run forward pass through the network\n",
    "            output = model(images)\n",
    "\n",
    "            # reset graph gradients\n",
    "            model.zero_grad()\n",
    "\n",
    "            # determine classification loss\n",
    "            loss = nll_loss(output, labels)\n",
    "\n",
    "            # run backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            # update network parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            # collect mini-batch reconstruction loss\n",
    "            train_mini_batch_losses.append(loss.data.item())\n",
    "\n",
    "        # determine mean min-batch loss of epoch\n",
    "        train_epoch_loss = np.mean(train_mini_batch_losses)\n",
    "\n",
    "        # print epoch loss\n",
    "        now = datetime.utcnow().strftime(\"%Y%m%d-%H:%M:%S\")\n",
    "        print('[LOG {}] epoch: {} train-loss: {}'.format(str(now), str(epoch), str(train_epoch_loss)))\n",
    "\n",
    "        # set filename of actual model\n",
    "        model_name = f'{str(model.__class__.__name__)}_model_e{num_epochs}_mb{mini_batch_size}_lr{learning_rate*1000}_epoch_{epoch}.pth'\n",
    "\n",
    "        # save model to local directory\n",
    "        torch.save(model.state_dict(), os.path.join(models_directory, model_name))\n",
    "\n",
    "        # determine mean min-batch loss of epoch\n",
    "        train_epoch_losses.append(train_epoch_loss)\n",
    "\n",
    "        # assign the index of the epoch with the lowest loss\n",
    "        min_epoch = np.argmin(train_epoch_losses)\n",
    "        min_loss = np.min(train_epoch_losses)\n",
    "\n",
    "    return str(model.__class__.__name__), train_epoch_losses, min_epoch, min_loss"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# function to plot the training epochs vs. the epochs' classification error\n",
    "def plot_training_loss(train_epoch_losses):\n",
    "    # prepare plot\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    # add grid\n",
    "    ax.grid(linestyle='dotted')\n",
    "\n",
    "    # plot the training epochs vs. the epochs' classification error\n",
    "    ax.plot(np.array(range(1, len(train_epoch_losses)+1)), train_epoch_losses, label='epoch loss (blue)')\n",
    "\n",
    "    # add axis legends\n",
    "    ax.set_xlabel(\"[training epoch $e_i$]\", fontsize=10)\n",
    "    ax.set_ylabel(\"[Classification Error $\\mathcal{L}^{NLL}$]\", fontsize=10)\n",
    "\n",
    "    # set plot legend\n",
    "    plt.legend(loc=\"upper right\", numpoints=1, fancybox=True)\n",
    "\n",
    "    # add plot title\n",
    "    plt.title('Training Epochs $e_i$ vs. Classification Error $L^{NLL}$', fontsize=10)\n",
    "    pass"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# function for evaluating the trained model\n",
    "def model_evaluation(model, path_to_best_model:str, eval_data, batch_size:int):\n",
    "    best_model = model\n",
    "\n",
    "    # load pre-trained models\n",
    "    best_model.load_state_dict(torch.load(path_to_best_model))\n",
    "\n",
    "    # set model in evaluation mode\n",
    "    best_model.eval()\n",
    "\n",
    "    # activate DataLoader for evaluation dataset\n",
    "    fashion_mnist_eval_dataloader = torch.utils.data.DataLoader(eval_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # init collection of mini-batch losses\n",
    "    eval_mini_batch_losses = []\n",
    "    nll_loss = nn.NLLLoss()\n",
    "\n",
    "    # iterate over all-mini batches\n",
    "    for i, (images, labels) in enumerate(fashion_mnist_eval_dataloader):\n",
    "\n",
    "        # run forward pass through the network\n",
    "        output = best_model(images)\n",
    "\n",
    "        # determine classification loss\n",
    "        loss = nll_loss(output, labels)\n",
    "\n",
    "        # collect mini-batch reconstruction loss\n",
    "        eval_mini_batch_losses.append(loss.data.item())\n",
    "\n",
    "    # determine mean min-batch loss of epoch\n",
    "    eval_loss = np.mean(eval_mini_batch_losses)\n",
    "\n",
    "    # print epoch loss\n",
    "    now = datetime.utcnow().strftime(\"%Y%m%d-%H:%M:%S\")\n",
    "    print('[LOG {}] eval-loss: {}'.format(str(now), str(eval_loss)))\n",
    "\n",
    "    predictions = torch.argmax(best_model(next(iter(fashion_mnist_eval_dataloader))[0]), dim=1)\n",
    "    metrics.accuracy_score(fashion_mnist_eval_data.targets, predictions.detach())\n",
    "\n",
    "    # determine classification matrix of the predicted and target classes\n",
    "    mat = confusion_matrix(fashion_mnist_eval_data.targets, predictions.detach())\n",
    "\n",
    "    # initialize the plot and define size\n",
    "    plt.figure(figsize=(8, 8))\n",
    "\n",
    "    # plot corresponding confusion matrix\n",
    "    sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False, cmap='YlOrRd_r', xticklabels=fashion_classes.values(), yticklabels=fashion_classes.values())\n",
    "    plt.tick_params(axis='both', which='major', labelsize=8, labelbottom = False, bottom=False, top = False, left = False, labeltop=True)\n",
    "\n",
    "    # set plot title\n",
    "    plt.title('Fashion-MNIST classification matrix')\n",
    "\n",
    "    # set plot axis lables\n",
    "    plt.xlabel('[true label]')\n",
    "    plt.ylabel('[predicted label]')\n",
    "\n",
    "    return eval_loss, mat"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Environment setup and data loading"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] running locally\n"
     ]
    }
   ],
   "source": [
    "# check the environment\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    RUN_IN_COLAB = True\n",
    "    print('[LOG] running on Google Colab')\n",
    "else:\n",
    "    RUN_IN_COLAB = False\n",
    "    print('[LOG] running locally')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-26T13:30:54.268611Z",
     "start_time": "2023-06-26T13:30:54.263708Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "if RUN_IN_COLAB:\n",
    "    # import the Google Colab GDrive connector\n",
    "    from google.colab import drive\n",
    "\n",
    "    # mount GDrive inside the Colab notebook\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    # create Colab Notebooks directory\n",
    "    notebook_directory = '/content/drive/MyDrive/Colab Notebooks'\n",
    "    if not os.path.exists(notebook_directory): os.makedirs(notebook_directory)\n",
    "\n",
    "    # create data sub-directory inside the Colab Notebooks directory\n",
    "    data_directory = '/content/drive/MyDrive/Colab Notebooks/data_fmnist'\n",
    "    if not os.path.exists(data_directory): os.makedirs(data_directory)\n",
    "\n",
    "    # create models sub-directory inside the Colab Notebooks directory\n",
    "    models_directory = '/content/drive/MyDrive/Colab Notebooks/models_fmnist'\n",
    "    if not os.path.exists(models_directory): os.makedirs(models_directory)\n",
    "else:\n",
    "    # create the data sub-directory\n",
    "    data_directory = './data_fmnist'\n",
    "    if not os.path.exists(data_directory): os.makedirs(data_directory)\n",
    "\n",
    "    # create the models sub-directory\n",
    "    models_directory = './models_fmnist'\n",
    "    if not os.path.exists(models_directory): os.makedirs(models_directory)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kd48p1W5LRyR",
    "outputId": "f140d2e8-f737-4e27-fa1d-7513226df255",
    "ExecuteTime": {
     "end_time": "2023-06-26T13:30:54.273075Z",
     "start_time": "2023-06-26T13:30:54.268836Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "# initialize the seed\n",
    "seed = 42\n",
    "np.random.seed(seed)"
   ],
   "metadata": {
    "id": "3Euf3lWrLRyS",
    "ExecuteTime": {
     "end_time": "2023-06-26T13:30:54.276602Z",
     "start_time": "2023-06-26T13:30:54.273458Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] notebook with mps computation enabled\n"
     ]
    }
   ],
   "source": [
    "# select the device to be used for training\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu').type\n",
    "\n",
    "# init seed for every device\n",
    "torch.manual_seed(seed) # cpu\n",
    "torch.cuda.manual_seed(seed) # gpu\n",
    "torch.mps.manual_seed(seed) # mps\n",
    "\n",
    "# log type of device enabled\n",
    "print('[LOG] notebook with {} computation enabled'.format(str(device)))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rDZHVYQPLRyS",
    "outputId": "31836e7f-8def-45dc-9454-8c3332011906",
    "ExecuteTime": {
     "end_time": "2023-06-26T13:30:54.283317Z",
     "start_time": "2023-06-26T13:30:54.279121Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "# specify training path for loading\n",
    "train_path = data_directory + '/train_fmnist'"
   ],
   "metadata": {
    "id": "B3C3pt9fLRyT",
    "ExecuteTime": {
     "end_time": "2023-06-26T13:30:54.285895Z",
     "start_time": "2023-06-26T13:30:54.282907Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "# define pytorch transformation into tensor format\n",
    "transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "\n",
    "# download and transform training images\n",
    "fashion_mnist_train_data = torchvision.datasets.FashionMNIST(root=train_path, train=True, download=True, transform=transform)"
   ],
   "metadata": {
    "id": "2OD2E-g_LRyT",
    "ExecuteTime": {
     "end_time": "2023-06-26T13:30:54.319910Z",
     "start_time": "2023-06-26T13:30:54.286424Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "# define fashion mnist classes\n",
    "fashion_classes = {0: 'T-shirt/top',\n",
    "                    1: 'Trouser',\n",
    "                    2: 'Pullover',\n",
    "                    3: 'Dress',\n",
    "                    4: 'Coat',\n",
    "                    5: 'Sandal',\n",
    "                    6: 'Shirt',\n",
    "                    7: 'Sneaker',\n",
    "                    8: 'Bag',\n",
    "                    9: 'Ankle boot'}"
   ],
   "metadata": {
    "id": "0L73MVkpLRyT",
    "ExecuteTime": {
     "end_time": "2023-06-26T13:30:54.329322Z",
     "start_time": "2023-06-26T13:30:54.304142Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "eval_path = data_directory + '/eval_fmnist'\n",
    "\n",
    "# define pytorch transformation into tensor format\n",
    "transf = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "\n",
    "# download and transform training images\n",
    "fashion_mnist_eval_data = torchvision.datasets.FashionMNIST(root=eval_path, train=False, transform=transf, download=True)"
   ],
   "metadata": {
    "id": "cJ8yKiqhLRyV",
    "ExecuteTime": {
     "end_time": "2023-06-26T13:30:54.329629Z",
     "start_time": "2023-06-26T13:30:54.320154Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "# len(fashion_mnist_eval_data)"
   ],
   "metadata": {
    "id": "W_jxj4crLRyV",
    "outputId": "84bc883d-f705-42c1-8722-563c5c3382a8",
    "ExecuteTime": {
     "end_time": "2023-06-26T13:30:54.329683Z",
     "start_time": "2023-06-26T13:30:54.320377Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For drawing: [http://alexlenail.me/NN-SVG/LeNet.html](http://alexlenail.me/NN-SVG/LeNet.html)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "class IaroHasNoFashionNet1(nn.Module):\n",
    "\n",
    "    # define the class constructor\n",
    "    def __init__(self):\n",
    "\n",
    "        # call super class constructor\n",
    "        super(IaroHasNoFashionNet1, self).__init__()\n",
    "\n",
    "        # specify convolution layer 1\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=3, kernel_size=5, stride=1, padding=0)\n",
    "\n",
    "        # define max-pooling layer 1\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # specify fully-connected (fc) layer 1 - in 12*12*3, out 48\n",
    "        self.linear1 = nn.Linear(12*12*3, 48, bias=True) # the linearity W*x+b\n",
    "        self.relu1 = nn.ReLU(inplace=True) # the non-linearity\n",
    "\n",
    "        # specify fc layer 2 - in 48, out 10\n",
    "        self.linear2 = nn.Linear(48, 10, bias=True) # the linearity W*x+b\n",
    "\n",
    "        # add a softmax to the last layer\n",
    "        self.logsoftmax = nn.LogSoftmax(dim=1) # the softmax\n",
    "\n",
    "    # define network forward pass\n",
    "    def forward(self, images):\n",
    "        # high-level feature learning via convolutional layers\n",
    "\n",
    "        # define conv layer 1 forward pass\n",
    "        x = self.pool1(self.relu1(self.conv1(images)))\n",
    "\n",
    "        #print(x.shape)\n",
    "\n",
    "        # reshape image pixels\n",
    "        x = x.view(-1, 12*12*3)\n",
    "\n",
    "        # define fc layer 1 forward pass\n",
    "        x = self.relu1(self.linear1(x))\n",
    "\n",
    "        # define layer 2 forward pass\n",
    "        x = self.logsoftmax(self.linear2(x))\n",
    "\n",
    "        # return forward pass result\n",
    "        return x"
   ],
   "metadata": {
    "id": "BPosxMtwLRyV",
    "ExecuteTime": {
     "end_time": "2023-06-26T13:30:54.329723Z",
     "start_time": "2023-06-26T13:30:54.320412Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model 1 Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] Training IaroHasNoFashionNet1...\n",
      "[LOG] Model architecture:\n",
      "\n",
      "IaroHasNoFashionNet1(\n",
      "  (conv1): Conv2d(1, 3, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (linear1): Linear(in_features=432, out_features=48, bias=True)\n",
      "  (relu1): ReLU(inplace=True)\n",
      "  (linear2): Linear(in_features=48, out_features=10, bias=True)\n",
      "  (logsoftmax): LogSoftmax(dim=1)\n",
      ")\n",
      "\n",
      "[LOG] Number of to be trained IaroHasNoFashionNet1 model parameters: 21352\n",
      "\n",
      "[LOG 20230626-13:31:22] epoch: 0 train-loss: 1.6118353170394897\n",
      "[LOG 20230626-13:31:50] epoch: 1 train-loss: 0.7609997013231119\n",
      "[LOG 20230626-13:32:18] epoch: 2 train-loss: 0.6491446369104087\n",
      "[LOG 20230626-13:32:45] epoch: 3 train-loss: 0.5992002764955163\n",
      "[LOG 20230626-13:33:12] epoch: 4 train-loss: 0.5701982664634784\n",
      "[LOG 20230626-13:33:39] epoch: 5 train-loss: 0.5480478898800909\n",
      "[LOG 20230626-13:34:07] epoch: 6 train-loss: 0.5297159824572504\n",
      "[LOG 20230626-13:34:34] epoch: 7 train-loss: 0.5142636025480926\n",
      "[LOG 20230626-13:35:01] epoch: 8 train-loss: 0.4999303490057588\n",
      "[LOG 20230626-13:35:28] epoch: 9 train-loss: 0.4865157442719986\n",
      "[LOG 20230626-13:35:55] epoch: 10 train-loss: 0.47563927085772156\n",
      "[LOG 20230626-13:36:22] epoch: 11 train-loss: 0.46614416262122493\n",
      "[LOG 20230626-13:36:49] epoch: 12 train-loss: 0.45709412678610534\n",
      "[LOG 20230626-13:37:16] epoch: 13 train-loss: 0.44952921109826616\n",
      "[LOG 20230626-13:37:44] epoch: 14 train-loss: 0.4430364915593217\n",
      "[LOG 20230626-13:38:11] epoch: 15 train-loss: 0.4367620924577117\n",
      "[LOG 20230626-13:38:39] epoch: 16 train-loss: 0.4315262478311236\n",
      "[LOG 20230626-13:39:08] epoch: 17 train-loss: 0.4268860910512507\n",
      "[LOG 20230626-13:39:36] epoch: 18 train-loss: 0.42085057569829126\n",
      "[LOG 20230626-13:40:04] epoch: 19 train-loss: 0.4165619548636799\n",
      "[LOG 20230626-13:40:32] epoch: 20 train-loss: 0.4129606168272129\n",
      "[LOG 20230626-13:41:00] epoch: 21 train-loss: 0.4087674208705934\n",
      "[LOG 20230626-13:41:28] epoch: 22 train-loss: 0.4027573637261987\n",
      "[LOG 20230626-13:41:54] epoch: 23 train-loss: 0.39916125004769615\n",
      "[LOG 20230626-13:42:17] epoch: 24 train-loss: 0.39607088468149304\n",
      "[LOG 20230626-13:42:39] epoch: 25 train-loss: 0.39273360305810345\n",
      "[LOG 20230626-13:43:01] epoch: 26 train-loss: 0.38886416482124475\n",
      "[LOG 20230626-13:43:23] epoch: 27 train-loss: 0.385311576924442\n",
      "[LOG 20230626-13:43:45] epoch: 28 train-loss: 0.38184404530494165\n",
      "[LOG 20230626-13:44:07] epoch: 29 train-loss: 0.37922118261115006\n",
      "[LOG 20230626-13:44:29] epoch: 30 train-loss: 0.3762948732985494\n",
      "[LOG 20230626-13:44:51] epoch: 31 train-loss: 0.3735712807332476\n",
      "[LOG 20230626-13:45:13] epoch: 32 train-loss: 0.3708528800579719\n",
      "[LOG 20230626-13:45:35] epoch: 33 train-loss: 0.3680244233574718\n",
      "[LOG 20230626-13:45:58] epoch: 34 train-loss: 0.3651076175459971\n",
      "[LOG 20230626-13:46:20] epoch: 35 train-loss: 0.3625473821544709\n",
      "[LOG 20230626-13:46:42] epoch: 36 train-loss: 0.3596265296836073\n",
      "[LOG 20230626-13:47:04] epoch: 37 train-loss: 0.35841811670252743\n",
      "[LOG 20230626-13:47:26] epoch: 38 train-loss: 0.3547427842228984\n",
      "[LOG 20230626-13:47:48] epoch: 39 train-loss: 0.35255772761718057\n",
      "[LOG 20230626-13:48:10] epoch: 40 train-loss: 0.3507796726036196\n",
      "[LOG 20230626-13:48:32] epoch: 41 train-loss: 0.34820324786389245\n",
      "[LOG 20230626-13:48:54] epoch: 42 train-loss: 0.3461068345896279\n",
      "[LOG 20230626-13:49:16] epoch: 43 train-loss: 0.34346135552177826\n",
      "[LOG 20230626-13:49:38] epoch: 44 train-loss: 0.34189739809340486\n",
      "[LOG 20230626-13:50:00] epoch: 45 train-loss: 0.3395297880759618\n",
      "[LOG 20230626-13:50:22] epoch: 46 train-loss: 0.3372918910996988\n",
      "[LOG 20230626-13:50:44] epoch: 47 train-loss: 0.33549758844900257\n",
      "[LOG 20230626-13:51:06] epoch: 48 train-loss: 0.3332661497054311\n",
      "[LOG 20230626-13:51:28] epoch: 49 train-loss: 0.33271307032266634\n",
      "[LOG 20230626-13:51:50] epoch: 50 train-loss: 0.33004335888281155\n",
      "[LOG 20230626-13:52:13] epoch: 51 train-loss: 0.328333180713576\n",
      "[LOG 20230626-13:52:35] epoch: 52 train-loss: 0.326778081203904\n",
      "[LOG 20230626-13:52:57] epoch: 53 train-loss: 0.32502194488439706\n",
      "[LOG 20230626-13:53:19] epoch: 54 train-loss: 0.32300030400818214\n",
      "[LOG 20230626-13:53:41] epoch: 55 train-loss: 0.3213619209352881\n",
      "[LOG 20230626-13:54:03] epoch: 56 train-loss: 0.3198854505491909\n",
      "[LOG 20230626-13:54:25] epoch: 57 train-loss: 0.31852835472244767\n",
      "[LOG 20230626-13:54:47] epoch: 58 train-loss: 0.31647081395580123\n",
      "[LOG 20230626-13:55:09] epoch: 59 train-loss: 0.31521403798280906\n",
      "[LOG 20230626-13:55:31] epoch: 60 train-loss: 0.3133806025649887\n",
      "[LOG 20230626-13:55:53] epoch: 61 train-loss: 0.3126523589244268\n",
      "[LOG 20230626-13:56:15] epoch: 62 train-loss: 0.3113304828187451\n",
      "[LOG 20230626-13:56:38] epoch: 63 train-loss: 0.3102243175870894\n",
      "[LOG 20230626-13:57:05] epoch: 64 train-loss: 0.30820736233470963\n",
      "[LOG 20230626-13:57:29] epoch: 65 train-loss: 0.3071506516349502\n",
      "[LOG 20230626-13:57:53] epoch: 66 train-loss: 0.3059549275147418\n",
      "[LOG 20230626-13:58:17] epoch: 67 train-loss: 0.3042655952173906\n",
      "[LOG 20230626-13:58:41] epoch: 68 train-loss: 0.3029039741971685\n",
      "[LOG 20230626-13:59:06] epoch: 69 train-loss: 0.3021653832586482\n",
      "[LOG 20230626-13:59:30] epoch: 70 train-loss: 0.3009321275634225\n",
      "[LOG 20230626-13:59:54] epoch: 71 train-loss: 0.300052472248914\n",
      "[LOG 20230626-14:00:18] epoch: 72 train-loss: 0.29873185523111995\n",
      "[LOG 20230626-14:00:44] epoch: 73 train-loss: 0.29721606263760475\n",
      "[LOG 20230626-14:01:11] epoch: 74 train-loss: 0.29633812738054743\n",
      "[LOG 20230626-14:01:38] epoch: 75 train-loss: 0.2949736662845515\n",
      "[LOG 20230626-14:02:05] epoch: 76 train-loss: 0.2942382860541266\n",
      "[LOG 20230626-14:02:32] epoch: 77 train-loss: 0.2931503752837268\n",
      "[LOG 20230626-14:02:59] epoch: 78 train-loss: 0.2919966246839147\n",
      "[LOG 20230626-14:03:26] epoch: 79 train-loss: 0.2909545128486895\n",
      "[LOG 20230626-14:03:53] epoch: 80 train-loss: 0.29046616716847445\n",
      "[LOG 20230626-14:04:20] epoch: 81 train-loss: 0.28897677998963434\n",
      "[LOG 20230626-14:04:47] epoch: 82 train-loss: 0.2872119358190568\n",
      "[LOG 20230626-14:05:13] epoch: 83 train-loss: 0.286748413968614\n",
      "[LOG 20230626-14:05:40] epoch: 84 train-loss: 0.28570178712946365\n",
      "[LOG 20230626-14:06:07] epoch: 85 train-loss: 0.28521864600486585\n",
      "[LOG 20230626-14:06:34] epoch: 86 train-loss: 0.28382752622306967\n",
      "[LOG 20230626-14:07:00] epoch: 87 train-loss: 0.28324725487378116\n",
      "[LOG 20230626-14:07:27] epoch: 88 train-loss: 0.2820881720121174\n",
      "[LOG 20230626-14:07:54] epoch: 89 train-loss: 0.2814797656900249\n",
      "[LOG 20230626-14:08:20] epoch: 90 train-loss: 0.27999315652456136\n",
      "[LOG 20230626-14:08:47] epoch: 91 train-loss: 0.27922369810106856\n",
      "[LOG 20230626-14:09:14] epoch: 92 train-loss: 0.27820418385208273\n",
      "[LOG 20230626-14:09:41] epoch: 93 train-loss: 0.2779529956347775\n",
      "[LOG 20230626-14:10:08] epoch: 94 train-loss: 0.27633444905548044\n",
      "[LOG 20230626-14:10:35] epoch: 95 train-loss: 0.275365977248208\n",
      "[LOG 20230626-14:11:00] epoch: 96 train-loss: 0.2750965274005275\n",
      "[LOG 20230626-14:11:23] epoch: 97 train-loss: 0.2738245494850135\n",
      "[LOG 20230626-14:11:46] epoch: 98 train-loss: 0.27263743551344766\n",
      "[LOG 20230626-14:12:08] epoch: 99 train-loss: 0.2722451721699288\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/99/8fqp9kbj5bqb2ry4s1tqkw5h0000gp/T/ipykernel_50524/2588016675.py\u001B[0m in \u001B[0;36m?\u001B[0;34m()\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0mm1_training_results\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mDataFrame\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcolumns\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"name,epochs,lr,mbs,loss,min_epoch,min_loss\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msplit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\",\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0min_model1\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mIaroHasNoFashionNet1\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[0mbatch_size\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m10000\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mlr\u001B[0m \u001B[0;32min\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;36m0.0005\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m0.001\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m0.005\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m0.01\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 8\u001B[0;31m     \u001B[0;32mfor\u001B[0m \u001B[0mmini_batch_size\u001B[0m \u001B[0;32min\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;36m8\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m16\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m32\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m64\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m128\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m256\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m512\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      9\u001B[0m         \u001B[0mnum_epochs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m100\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     10\u001B[0m         \u001B[0mm1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrain_epoch_losses1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmin_epoch1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmin_loss1\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmodel_training\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0min_model1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdevice\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlr\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnum_epochs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmini_batch_size\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfashion_mnist_train_data\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmodels_directory\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     11\u001B[0m         \u001B[0mm1_training_results\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mm1_training_results\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m{\u001B[0m\u001B[0;34m'name'\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0;34m'FashionMNISTNet1'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'epochs'\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mnum_epochs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'lr'\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mlr\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'mbs'\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mmini_batch_size\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'loss'\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mtrain_epoch_losses1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'min_epoch'\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mmin_epoch1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'min_loss'\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mmin_loss1\u001B[0m\u001B[0;34m}\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mignore_index\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/courses/gserm/venv/lib/python3.9/site-packages/pandas/core/generic.py\u001B[0m in \u001B[0;36m?\u001B[0;34m(self, name)\u001B[0m\n\u001B[1;32m   5985\u001B[0m             \u001B[0;32mand\u001B[0m \u001B[0mname\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_accessors\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   5986\u001B[0m             \u001B[0;32mand\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_info_axis\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_can_hold_identifiers_and_holds_name\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   5987\u001B[0m         ):\n\u001B[1;32m   5988\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 5989\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mobject\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__getattribute__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m: 'DataFrame' object has no attribute 'append'"
     ]
    }
   ],
   "source": [
    "m1_training_results = []\n",
    "in_model1 = IaroHasNoFashionNet1()\n",
    "batch_size = 10000\n",
    "for lr in [0.0005, 0.001, 0.005, 0.01]:\n",
    "    for mini_batch_size in [8, 16, 32, 64, 128, 256, 512]:\n",
    "        num_epochs = 100\n",
    "        m1, train_epoch_losses1, min_epoch1, min_loss1 = model_training(in_model1, device, lr, num_epochs, mini_batch_size, fashion_mnist_train_data, models_directory)\n",
    "        m1_training_results = m1_training_results.append({'name': 'FashionMNISTNet1', 'epochs': num_epochs, 'lr': lr, 'mbs': mini_batch_size, 'loss': train_epoch_losses1, 'min_epoch': min_epoch1, 'min_loss': min_loss1}, ignore_index=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-26T14:12:08.157238Z",
     "start_time": "2023-06-26T13:30:54.323314Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "m1_training_results.to_csv('./models_fmnist/IaroHasNoFashionNet1_training_results.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-26T14:12:08.148899Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "m1_training_results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_training_loss(train_epoch_losses1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# init pre-trained model class\n",
    "best_model = IaroHasNoFashionNet1()\n",
    "\n",
    "# path_to_best_model = '/content/drive/MyDrive/Colab Notebooks/models_fmnist/FashionMNIST10_model_epoch_19.pth'\n",
    "\n",
    "eval_loss1, _ = model_evaluation(best_model, './models_fmnist/FashionMNISTNet1_model_e10_mb512_lr1.0_epoch_9.pth', fashion_mnist_eval_data, batch_size)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # specify training path for loading\n",
    "# train_path = data_directory + '/train_fmnist'\n",
    "# # define pytorch transformation into tensor format\n",
    "# transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "#\n",
    "# # download and transform training images\n",
    "# fashion_mnist_train_data = torchvision.datasets.FashionMNIST(root=train_path, train=True, download=True,\n",
    "#                                                              transform=transform)\n",
    "# # define fashion mnist classes\n",
    "# fashion_classes = {0: 'T-shirt/top',\n",
    "#                    1: 'Trouser',\n",
    "#                    2: 'Pullover',\n",
    "#                    3: 'Dress',\n",
    "#                    4: 'Coat',\n",
    "#                    5: 'Sandal',\n",
    "#                    6: 'Shirt',\n",
    "#                    7: 'Sneaker',\n",
    "#                    8: 'Bag',\n",
    "#                    9: 'Ankle boot'}\n",
    "# eval_path = data_directory + '/eval_fmnist'\n",
    "#\n",
    "# # define pytorch transformation into tensor format\n",
    "# transf = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "#\n",
    "# # download and transform training images\n",
    "# fashion_mnist_eval_data = torchvision.datasets.FashionMNIST(root=eval_path, train=False, transform=transf, download=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class IaroHasNoFashionNet2(nn.Module):\n",
    "\n",
    "    # define the class constructor\n",
    "    def __init__(self):\n",
    "\n",
    "        # call super class constructor\n",
    "        super(IaroHasNoFashionNet2, self).__init__()\n",
    "\n",
    "        # specify convolution layer 1\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=3, kernel_size=5, stride=1, padding=0)\n",
    "\n",
    "        # define max-pooling layer 1\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # specify fully-connected (fc) layer 1 - in 12*12*3, out 48\n",
    "        self.linear1 = nn.Linear(12*12*3, 48, bias=True) # the linearity W*x+b\n",
    "        self.relu1 = nn.ReLU(inplace=True) # the non-linearity\n",
    "\n",
    "        # specify fc layer 2 - in 48, out 10\n",
    "        self.linear2 = nn.Linear(48, 10, bias=True) # the linearity W*x+b\n",
    "\n",
    "        # add a softmax to the last layer\n",
    "        self.logsoftmax = nn.LogSoftmax(dim=1) # the softmax\n",
    "\n",
    "    # define network forward pass\n",
    "    def forward(self, images):\n",
    "        # high-level feature learning via convolutional layers\n",
    "\n",
    "        # define conv layer 1 forward pass\n",
    "        x = self.pool1(self.relu1(self.conv1(images)))\n",
    "\n",
    "        #print(x.shape)\n",
    "\n",
    "        # reshape image pixels\n",
    "        x = x.view(-1, 12*12*3)\n",
    "\n",
    "        # define fc layer 1 forward pass\n",
    "        x = self.relu1(self.linear1(x))\n",
    "\n",
    "        # define layer 2 forward pass\n",
    "        x = self.logsoftmax(self.linear2(x))\n",
    "\n",
    "        # return forward pass result\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "in_model2 = IaroHasNoFashionNet2()\n",
    "lr2 = 0.001\n",
    "num_epochs2 = 10\n",
    "mini_batch_size2 = 64\n",
    "batch_size2 = 10000\n",
    "m2, min_epoch2, train_epoch_losses2 = model_training(in_model2, device, lr2, num_epochs2, mini_batch_size2, fashion_mnist_train_data, models_directory)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# init pre-trained model class\n",
    "best_model2 = IaroHasNoFashionNet2()\n",
    "\n",
    "# path_to_best_model = '/content/drive/MyDrive/Colab Notebooks/models_fmnist/FashionMNIST10_model_epoch_19.pth'\n",
    "\n",
    "eval_loss2, _ = model_evaluation(best_model2, './models_fmnist/IaroHasNoFashionNet2_model_e10_mb64_lr1.0_epoch_9.pth', fashion_mnist_eval_data, batch_size2)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
